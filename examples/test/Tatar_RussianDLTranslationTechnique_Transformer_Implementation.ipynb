{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tatar-RussianDLTranslationTechnique Transformer Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wavatatarskii/TatarDLTranslationTechniques/blob/master/examples/test/Tatar_RussianDLTranslationTechnique_Transformer_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZ6uijDCGVi",
        "colab_type": "text"
      },
      "source": [
        "### Install the nightly built Tensorflow 2.0 to use LayerNorm layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mApbMiIBsmIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "510193b3-99e3-4a6e-f889-1de9917af216"
      },
      "source": [
        "!git clone https://github.com/wavatatarskii/TatarDLTranslationTechniques.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TatarDLTranslationTechniques'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  20% (1/5)\u001b[K\rremote: Compressing objects:  40% (2/5)\u001b[K\rremote: Compressing objects:  60% (3/5)\u001b[K\rremote: Compressing objects:  80% (4/5)\u001b[K\rremote: Compressing objects: 100% (5/5)\u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "Unpacking objects:  16% (1/6)   \rUnpacking objects:  33% (2/6)   \rUnpacking objects:  50% (3/6)   \rUnpacking objects:  66% (4/6)   \rremote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  83% (5/6)   \rUnpacking objects: 100% (6/6)   \rUnpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kElTdoawPxzb",
        "colab_type": "code",
        "outputId": "fe860216-4756-44d6-9028-b4ce9ce08652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# !pip install tensorflow-gpu==2.0.0-alpha\n",
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-gpu-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190506)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0a20190506)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0.dev2019050600)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfoKFO4lCOzU",
        "colab_type": "text"
      },
      "source": [
        "### Import and create utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtTr3Pmr9dMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "efdc3330-3c5b-4acf-c3db-ad7dee219832"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "import time\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# Mode can be either 'train' or 'infer'\n",
        "# Set to 'infer' will skip the training\n",
        "MODE = 'train'\n",
        "# URL = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "#URL = 'http://www.manythings.org/anki/ita-eng.zip'\n",
        "FILENAME = '/content/TatarDLTranslationTechniques/tatTABru60000-1.zip'\n",
        "\n",
        "\n",
        "def maybe_download_and_read_file(url, filename):\n",
        "    \"\"\" Download and unzip training data\n",
        "    Args:\n",
        "        url: data url\n",
        "        filename: zip filename\n",
        "    \n",
        "    Returns:\n",
        "        Training data: an array containing text lines from the data\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        session = requests.Session()\n",
        "        response = session.get(url, stream=True)\n",
        "\n",
        "        CHUNK_SIZE = 32768\n",
        "        with open(filename, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    zipf = ZipFile(filename)\n",
        "    filename = zipf.namelist()\n",
        "    with zipf.open(filename[0]) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFCi9xR5CVCZ",
        "colab_type": "text"
      },
      "source": [
        "### Download and process training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnj3KncAwro",
        "colab_type": "code",
        "outputId": "853d1987-5b38-4cd5-c735-8034f26580cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "lines = maybe_download_and_read_file(None, FILENAME)\n",
        "lines = lines.decode('utf-8')\n",
        "\n",
        "raw_data = []\n",
        "for line in lines.split('\\n'):\n",
        "    raw_data.append(line.split('\\t'))\n",
        "\n",
        "print(raw_data[-5:])\n",
        "# The last element is empty, so omit it\n",
        "raw_data = raw_data[:-1]\n",
        "\n",
        "\n",
        "\"\"\"## Preprocessing\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "raw_data_tt, raw_data_ru = list(zip(*raw_data))\n",
        "raw_data_tt = [normalize_string(data) for data in raw_data_tt]\n",
        "raw_data_ru_in = ['<start> ' + normalize_string(data) for data in raw_data_ru]\n",
        "raw_data_ru_out = [normalize_string(data) + ' <end>' for data in raw_data_ru]\n",
        "\n",
        "\"\"\"## Tokenization\"\"\"\n",
        "\n",
        "tt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tt_tokenizer.fit_on_texts(raw_data_tt)\n",
        "data_tt = tt_tokenizer.texts_to_sequences(raw_data_tt)\n",
        "data_tt = tf.keras.preprocessing.sequence.pad_sequences(data_tt,\n",
        "                                                        padding='post')\n",
        "\n",
        "ru_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "ru_tokenizer.fit_on_texts(raw_data_ru_in)\n",
        "ru_tokenizer.fit_on_texts(raw_data_ru_out)\n",
        "data_ru_in = ru_tokenizer.texts_to_sequences(raw_data_ru_in)\n",
        "data_ru_in = tf.keras.preprocessing.sequence.pad_sequences(data_ru_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_ru_out = ru_tokenizer.texts_to_sequences(raw_data_ru_out)\n",
        "data_ru_out = tf.keras.preprocessing.sequence.pad_sequences(data_ru_out,\n",
        "                                                            padding='post')\n",
        "\n",
        "\"\"\"## Create tf.data.Dataset object\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_tt, data_ru_in, data_ru_out))\n",
        "dataset = dataset.shuffle(len(data_tt)).batch(BATCH_SIZE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Болай да артыгын сөйләдем.', 'Я и так изложил.\\r'], ['Рәхмәт, Мәннән абзый.', 'Спасибо, Майдан-абзый.\\r'], ['Сау бул, кызым.', 'До свидания, дочка.\\r'], ['Ана теләге дәрья төбеннән чыгарыр, диләр бит.', 'Говорят же, что желание матери выведет ее из дна.\\r'], ['Йоклаганга салышып, тын гына ята бирде.', 'Молча лежал, притворившись спящим.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RmU-u7CYwQ",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg7_8DsDAZoc",
        "colab_type": "code",
        "outputId": "b9faac72-94fc-4aa3-ddf9-7b11fd0da496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\"\"\"## Create the Positional Embedding\"\"\"\n",
        "\n",
        "\n",
        "def positional_encoding(pos, model_size):\n",
        "    \"\"\" Compute positional encoding for a particular position\n",
        "    Args:\n",
        "        pos: position of a token in the sequence\n",
        "        model_size: depth size of the model\n",
        "    \n",
        "    Returns:\n",
        "        The positional encoding for the given token\n",
        "    \"\"\"\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "        if i % 2 == 0:\n",
        "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "        else:\n",
        "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
        "    return PE\n",
        "\n",
        "max_length = max(len(data_tt[0]), len(data_ru_in[0]))\n",
        "MODEL_SIZE = 128\n",
        "\n",
        "pes = []\n",
        "for i in range(max_length):\n",
        "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)\n",
        "\n",
        "\n",
        "print(pes.shape)\n",
        "print(data_tt.shape)\n",
        "print(data_ru_in.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98, 128)\n",
            "(59734, 98)\n",
            "(59734, 90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJPr_lqxCbvi",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgug1s0wAeAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    \"\"\" Class for Multi-Head Attention layer\n",
        "    Attributes:\n",
        "        key_size: d_key in the paper\n",
        "        h: number of attention heads\n",
        "        wq: the Linear layer for Q\n",
        "        wk: the Linear layer for K\n",
        "        wv: the Linear layer for V\n",
        "        wo: the Linear layer for the output\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.key_size = model_size // h\n",
        "        self.h = h\n",
        "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def call(self, query, value, mask=None):\n",
        "        \"\"\" The forward pass for Multi-Head Attention layer\n",
        "        Args:\n",
        "            query: the Q matrix\n",
        "            value: the V matrix, acts as V and K\n",
        "            mask: mask to filter out unwanted tokens\n",
        "                  - zero mask: mask for padded tokens\n",
        "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
        "        \n",
        "        Returns:\n",
        "            The concatenated context vector\n",
        "            The alignment (attention) vectors of all heads\n",
        "        \"\"\"\n",
        "        # query has shape (batch, query_len, model_size)\n",
        "        # value has shape (batch, value_len, model_size)\n",
        "        query = self.wq(query)\n",
        "        key = self.wk(value)\n",
        "        value = self.wv(value)\n",
        "        \n",
        "        # Split matrices for multi-heads attention\n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        # Originally, query has shape (batch, query_len, model_size)\n",
        "        # We need to reshape to (batch, query_len, h, key_size)\n",
        "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
        "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        \n",
        "        # Do the same for key and value\n",
        "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        \n",
        "        # Compute the dot score\n",
        "        # and divide the score by square root of key_size (as stated in paper)\n",
        "        # (must convert key_size to float32 otherwise an error would occur)\n",
        "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
        "        # score will have shape of (batch, h, query_len, value_len)\n",
        "        \n",
        "        # Mask out the score if a mask is provided\n",
        "        # There are two types of mask:\n",
        "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
        "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
        "        if mask is not None:\n",
        "            score *= mask\n",
        "\n",
        "            # We want the masked out values to be zeros when applying softmax\n",
        "            # One way to accomplish that is assign them to a very large negative value\n",
        "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "        \n",
        "        # Alignment vector: (batch, h, query_len, value_len)\n",
        "        alignment = tf.nn.softmax(score, axis=-1)\n",
        "        \n",
        "        # Context vector: (batch, h, query_len, key_size)\n",
        "        context = tf.matmul(alignment, value)\n",
        "        \n",
        "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])\n",
        "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
        "        \n",
        "        # Apply one last full connected layer (WO)\n",
        "        heads = self.wo(context)\n",
        "        \n",
        "        return heads, alignment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdTPndICfEu",
        "colab_type": "text"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3f3nloAg5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Encoder\"\"\"\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Encoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention: array of Multi-Head Attention layers\n",
        "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
        "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "\n",
        "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, sequence, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Encoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        sub_in = embed_out\n",
        "        alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
        "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
        "            sub_out = sub_in + sub_out\n",
        "            sub_out = self.attention_norm[i](sub_out)\n",
        "            \n",
        "            alignments.append(alignment)\n",
        "            ffn_in = sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_in + ffn_out\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            sub_in = ffn_out\n",
        "\n",
        "        return ffn_out, alignments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pxeATn-Cxzv",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-iSrjyYA8ws",
        "colab_type": "code",
        "outputId": "ab94883d-aa07-4aba-ed7e-8f21f81a8715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "H = 8\n",
        "NUM_LAYERS = 4\n",
        "vocab_size = len(tt_tokenizer.word_index) + 1\n",
        "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "print(vocab_size)\n",
        "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
        "encoder_output, _ = encoder(sequence_in)\n",
        "encoder_output.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(1), Dimension(5), Dimension(128)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShJ6Z0mCvyo",
        "colab_type": "text"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eFjbmBAfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Decoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
        "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
        "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
        "        attention_mid: array of middle Multi-Head Attention layers\n",
        "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
        "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "        dense: Dense layer to compute final output\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Decoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            encoder_output: output of the Encoder (for computing middle attention)\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The bottom alignment (attention) vectors for all layers\n",
        "            The middle alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        bot_sub_in = embed_out\n",
        "        bot_alignments = []\n",
        "        mid_alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # BOTTOM MULTIHEAD SUB LAYER\n",
        "            seq_len = bot_sub_in.shape[1]\n",
        "\n",
        "            if training:\n",
        "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "            else:\n",
        "                mask = None\n",
        "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
        "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
        "            bot_sub_out = bot_sub_in + bot_sub_out\n",
        "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
        "            \n",
        "            bot_alignments.append(bot_alignment)\n",
        "\n",
        "            # MIDDLE MULTIHEAD SUB LAYER\n",
        "            mid_sub_in = bot_sub_out\n",
        "\n",
        "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
        "                mid_sub_in, encoder_output, encoder_mask)\n",
        "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
        "            mid_sub_out = mid_sub_out + mid_sub_in\n",
        "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
        "            \n",
        "            mid_alignments.append(mid_alignment)\n",
        "\n",
        "            # FFN\n",
        "            ffn_in = mid_sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_out + ffn_in\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            bot_sub_in = ffn_out\n",
        "\n",
        "        logits = self.dense(ffn_out)\n",
        "\n",
        "        return logits, bot_alignments, mid_alignments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19DfjeaxCilt",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLPEA30BDjT",
        "colab_type": "code",
        "outputId": "40202116-daf1-4d89-a649-5de08e4e1d86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab_size = len(ru_tokenizer.word_index) + 1\n",
        "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "sequence_in = tf.constant([[14, 24, 31, 0, 0]])\n",
        "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
        "decoder_output.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-7eda3fbc8b8a>:72: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(1), Dimension(5), Dimension(64643)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYEswwMC01W",
        "colab_type": "text"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CmFGkJ2BMOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "\n",
        "def loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVxUv2tC3MM",
        "colab_type": "text"
      },
      "source": [
        "### Learning rate scheduling and optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwgwnb-3BNhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Learning schedule for training the Transformer\n",
        "    Attributes:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        warmup_steps: number of warmup steps at the beginning\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, warmup_steps=4000):\n",
        "        super(WarmupThenDecaySchedule, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_term = tf.math.rsqrt(step)\n",
        "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n",
        "\n",
        "\n",
        "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(lr,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMvgwyoDC9ks",
        "colab_type": "text"
      },
      "source": [
        "### The predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqSWlTcBPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    \"\"\" Predict the output sentence for a given input sentence\n",
        "    Args:\n",
        "        test_source_text: input sentence (raw string)\n",
        "    \n",
        "    Returns:\n",
        "        The encoder's attention vectors\n",
        "        The decoder's bottom attention vectors\n",
        "        The decoder's middle attention vectors\n",
        "        The input string array (input sentence split by ' ')\n",
        "        The output string array\n",
        "    \"\"\"\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_tt[np.random.choice(len(raw_data_tt))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = tt_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
        "\n",
        "    de_input = tf.constant(\n",
        "        [[ru_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(ru_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
        "        # so we have to add the last predicted word to create a new input sequence\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        # TODO: get a nicer constraint for the sequence length!\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn4MBXCOC_jB",
        "colab_type": "text"
      },
      "source": [
        "### The train_step function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUJ7_Y5BSAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out):\n",
        "    \"\"\" Execute one training step (forward pass + backward pass)\n",
        "    Args:\n",
        "        source_seq: source sequences\n",
        "        target_seq_in: input target sequences (<start> + ...)\n",
        "        target_seq_out: output target sequences (... + <end>)\n",
        "    \n",
        "    Returns:\n",
        "        The loss value of the current pass\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
        "        # encoder_mask has shape (batch_size, source_len)\n",
        "        # we need to add two more dimensions in between\n",
        "        # to make it broadcastable when computing attention heads\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
        "\n",
        "        decoder_output, _, _ = decoder(\n",
        "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
        "\n",
        "        loss = loss_func(target_seq_out, decoder_output)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5H0UbfNDB4S",
        "colab_type": "text"
      },
      "source": [
        "### The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyXdcoI3BTz6",
        "colab_type": "code",
        "outputId": "9795259f-2971-4a07-87e9-09811a18ea71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_EPOCHS = 35\n",
        "import pandas as pd\n",
        "starttime = time.time()\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "        loss = train_step(source_seq, target_seq_in,\n",
        "                          target_seq_out)\n",
        "        if batch % 60 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
        "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
        "            starttime = time.time()\n",
        "            table = pd.DataFrame(index=time.time(), columns= ['Loss'])\n",
        "\n",
        "    try:\n",
        "        predict()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.0140 Elapsed time 0.57s\n",
            "Epoch 1 Batch 60 Loss 0.9570 Elapsed time 25.63s\n",
            "Epoch 1 Batch 120 Loss 0.8968 Elapsed time 25.93s\n",
            "Epoch 1 Batch 180 Loss 0.8501 Elapsed time 25.52s\n",
            "Epoch 1 Batch 240 Loss 0.7644 Elapsed time 25.50s\n",
            "Epoch 1 Batch 300 Loss 0.9342 Elapsed time 25.60s\n",
            "Epoch 1 Batch 360 Loss 0.8236 Elapsed time 25.60s\n",
            "Epoch 1 Batch 420 Loss 0.7742 Elapsed time 25.59s\n",
            "Epoch 1 Batch 480 Loss 0.8510 Elapsed time 25.55s\n",
            "Epoch 1 Batch 540 Loss 0.8630 Elapsed time 25.69s\n",
            "Epoch 1 Batch 600 Loss 0.7614 Elapsed time 25.63s\n",
            "Epoch 1 Batch 660 Loss 0.8535 Elapsed time 25.66s\n",
            "Epoch 1 Batch 720 Loss 0.6867 Elapsed time 25.61s\n",
            "Epoch 1 Batch 780 Loss 0.8249 Elapsed time 25.67s\n",
            "Epoch 1 Batch 840 Loss 0.7739 Elapsed time 25.66s\n",
            "Epoch 1 Batch 900 Loss 0.7445 Elapsed time 25.66s\n",
            "Яңа елны быел бездә каршы алабыз дип .\n",
            "[[108, 1683, 1484, 677, 141, 3285, 12, 1]]\n",
            "он не было не было . <end>\n",
            "Epoch 2 Batch 0 Loss 0.7774 Elapsed time 19.91s\n",
            "Epoch 2 Batch 60 Loss 0.7841 Elapsed time 25.62s\n",
            "Epoch 2 Batch 120 Loss 0.7679 Elapsed time 25.72s\n",
            "Epoch 2 Batch 180 Loss 0.7593 Elapsed time 25.68s\n",
            "Epoch 2 Batch 240 Loss 0.6845 Elapsed time 25.68s\n",
            "Epoch 2 Batch 300 Loss 0.8271 Elapsed time 25.75s\n",
            "Epoch 2 Batch 360 Loss 0.7373 Elapsed time 25.55s\n",
            "Epoch 2 Batch 420 Loss 0.6866 Elapsed time 25.62s\n",
            "Epoch 2 Batch 480 Loss 0.7635 Elapsed time 25.69s\n",
            "Epoch 2 Batch 540 Loss 0.7787 Elapsed time 25.66s\n",
            "Epoch 2 Batch 600 Loss 0.6876 Elapsed time 25.63s\n",
            "Epoch 2 Batch 660 Loss 0.7718 Elapsed time 25.81s\n",
            "Epoch 2 Batch 720 Loss 0.6263 Elapsed time 25.74s\n",
            "Epoch 2 Batch 780 Loss 0.7526 Elapsed time 25.54s\n",
            "Epoch 2 Batch 840 Loss 0.7036 Elapsed time 25.87s\n",
            "Epoch 2 Batch 900 Loss 0.6818 Elapsed time 25.95s\n",
            "Фатихәттәй әйткәнчә үк булмаса да, түшәмнәр, стеналар чыннан да шактый каралган шул .\n",
            "[[640, 10798, 104, 1467, 26, 38193, 8709, 393, 2, 156, 6021, 20, 1]]\n",
            "он не мог не мог быть и не мог бы не мог не мог\n",
            "Epoch 3 Batch 0 Loss 0.7133 Elapsed time 15.67s\n",
            "Epoch 3 Batch 60 Loss 0.7173 Elapsed time 26.09s\n",
            "Epoch 3 Batch 120 Loss 0.7107 Elapsed time 25.81s\n",
            "Epoch 3 Batch 180 Loss 0.7120 Elapsed time 26.12s\n",
            "Epoch 3 Batch 240 Loss 0.6348 Elapsed time 25.81s\n",
            "Epoch 3 Batch 300 Loss 0.7614 Elapsed time 25.90s\n",
            "Epoch 3 Batch 360 Loss 0.6823 Elapsed time 25.87s\n",
            "Epoch 3 Batch 420 Loss 0.6290 Elapsed time 25.77s\n",
            "Epoch 3 Batch 480 Loss 0.7054 Elapsed time 25.99s\n",
            "Epoch 3 Batch 540 Loss 0.7234 Elapsed time 25.87s\n",
            "Epoch 3 Batch 600 Loss 0.6425 Elapsed time 25.96s\n",
            "Epoch 3 Batch 660 Loss 0.7073 Elapsed time 25.78s\n",
            "Epoch 3 Batch 720 Loss 0.5696 Elapsed time 25.85s\n",
            "Epoch 3 Batch 780 Loss 0.6869 Elapsed time 25.91s\n",
            "Epoch 3 Batch 840 Loss 0.6454 Elapsed time 25.79s\n",
            "Epoch 3 Batch 900 Loss 0.6305 Elapsed time 25.90s\n",
            "Операциягә ул риза булмады, гомерем күп калмагандыр, газапланасым килми, диде .\n",
            "[[4274, 3, 644, 2213, 3202, 90, 23589, 38858, 2078, 21, 1]]\n",
            "он не мог не мог бы не может быть бы не только не только\n",
            "Epoch 4 Batch 0 Loss 0.6628 Elapsed time 15.45s\n",
            "Epoch 4 Batch 60 Loss 0.6562 Elapsed time 25.77s\n",
            "Epoch 4 Batch 120 Loss 0.6494 Elapsed time 25.83s\n",
            "Epoch 4 Batch 180 Loss 0.6526 Elapsed time 25.83s\n",
            "Epoch 4 Batch 240 Loss 0.5777 Elapsed time 26.11s\n",
            "Epoch 4 Batch 300 Loss 0.7242 Elapsed time 25.82s\n",
            "Epoch 4 Batch 360 Loss 0.6159 Elapsed time 25.92s\n",
            "Epoch 4 Batch 420 Loss 0.5717 Elapsed time 25.82s\n",
            "Epoch 4 Batch 480 Loss 0.6377 Elapsed time 25.79s\n",
            "Epoch 4 Batch 540 Loss 0.6698 Elapsed time 25.92s\n",
            "Epoch 4 Batch 600 Loss 0.5795 Elapsed time 25.92s\n",
            "Epoch 4 Batch 660 Loss 0.6492 Elapsed time 25.99s\n",
            "Epoch 4 Batch 720 Loss 0.5247 Elapsed time 25.90s\n",
            "Epoch 4 Batch 780 Loss 0.6384 Elapsed time 26.00s\n",
            "Epoch 4 Batch 840 Loss 0.5931 Elapsed time 25.97s\n",
            "Epoch 4 Batch 900 Loss 0.5786 Elapsed time 26.01s\n",
            "Туганда ул кеше иде; аякларын, кулларын үзе теләгәнчә селкетергә ихтыярлы, муенын теләгәнчә борудан, күзләре белән теләгән җиргә караудан аны берәү дә тыя алмый иде .\n",
            "[[13806, 3, 25, 5928, 53184, 431, 39, 4761, 53185, 53186, 5308, 4761, 53187, 214, 6, 1379, 299, 27391, 24, 689, 5, 3008, 185, 4, 1]]\n",
            "он не мог быть с ним и не зная, что он не мог быть\n",
            "Epoch 5 Batch 0 Loss 0.6100 Elapsed time 15.65s\n",
            "Epoch 5 Batch 60 Loss 0.6188 Elapsed time 26.09s\n",
            "Epoch 5 Batch 120 Loss 0.5960 Elapsed time 26.18s\n",
            "Epoch 5 Batch 180 Loss 0.6006 Elapsed time 26.03s\n",
            "Epoch 5 Batch 240 Loss 0.5377 Elapsed time 26.09s\n",
            "Epoch 5 Batch 300 Loss 0.6545 Elapsed time 26.01s\n",
            "Epoch 5 Batch 360 Loss 0.5679 Elapsed time 25.88s\n",
            "Epoch 5 Batch 420 Loss 0.5342 Elapsed time 25.85s\n",
            "Epoch 5 Batch 480 Loss 0.6071 Elapsed time 25.87s\n",
            "Epoch 5 Batch 540 Loss 0.6291 Elapsed time 25.81s\n",
            "Epoch 5 Batch 600 Loss 0.5307 Elapsed time 25.77s\n",
            "Epoch 5 Batch 660 Loss 0.5896 Elapsed time 25.80s\n",
            "Epoch 5 Batch 720 Loss 0.4789 Elapsed time 25.80s\n",
            "Epoch 5 Batch 780 Loss 0.5782 Elapsed time 25.74s\n",
            "Epoch 5 Batch 840 Loss 0.5473 Elapsed time 25.75s\n",
            "Epoch 5 Batch 900 Loss 0.5284 Elapsed time 25.74s\n",
            "Нәҗип, мольбертын күтәргән килеш, өйгә узды .\n",
            "[[5445, 43403, 1557, 1200, 360, 827, 1]]\n",
            "макаев подошел к окну . <end>\n",
            "Epoch 6 Batch 0 Loss 0.5534 Elapsed time 14.88s\n",
            "Epoch 6 Batch 60 Loss 0.5657 Elapsed time 25.74s\n",
            "Epoch 6 Batch 120 Loss 0.5402 Elapsed time 25.71s\n",
            "Epoch 6 Batch 180 Loss 0.5432 Elapsed time 25.87s\n",
            "Epoch 6 Batch 240 Loss 0.4841 Elapsed time 25.70s\n",
            "Epoch 6 Batch 300 Loss 0.6077 Elapsed time 25.75s\n",
            "Epoch 6 Batch 360 Loss 0.5178 Elapsed time 25.74s\n",
            "Epoch 6 Batch 420 Loss 0.4859 Elapsed time 25.78s\n",
            "Epoch 6 Batch 480 Loss 0.5586 Elapsed time 25.77s\n",
            "Epoch 6 Batch 540 Loss 0.5687 Elapsed time 25.72s\n",
            "Epoch 6 Batch 600 Loss 0.4994 Elapsed time 25.77s\n",
            "Epoch 6 Batch 660 Loss 0.5401 Elapsed time 25.72s\n",
            "Epoch 6 Batch 720 Loss 0.4402 Elapsed time 25.77s\n",
            "Epoch 6 Batch 780 Loss 0.5275 Elapsed time 25.79s\n",
            "Epoch 6 Batch 840 Loss 0.5013 Elapsed time 25.78s\n",
            "Epoch 6 Batch 900 Loss 0.4790 Elapsed time 25.76s\n",
            "Тәлгатькә, техникумга керермен, дип язган иде, ярар, анысы качмас, көтеп торыр әле .\n",
            "[[60156, 19739, 13703, 12, 572, 91, 891, 563, 13060, 346, 1701, 19, 1]]\n",
            "я не думал, что он будет ходить по сторонам, чтобы он не сможет прийти\n",
            "Epoch 7 Batch 0 Loss 0.5083 Elapsed time 15.36s\n",
            "Epoch 7 Batch 60 Loss 0.5155 Elapsed time 25.73s\n",
            "Epoch 7 Batch 120 Loss 0.5025 Elapsed time 25.75s\n",
            "Epoch 7 Batch 180 Loss 0.5116 Elapsed time 25.85s\n",
            "Epoch 7 Batch 240 Loss 0.4500 Elapsed time 25.71s\n",
            "Epoch 7 Batch 300 Loss 0.5483 Elapsed time 25.70s\n",
            "Epoch 7 Batch 360 Loss 0.4731 Elapsed time 25.87s\n",
            "Epoch 7 Batch 420 Loss 0.4374 Elapsed time 25.70s\n",
            "Epoch 7 Batch 480 Loss 0.5145 Elapsed time 25.78s\n",
            "Epoch 7 Batch 540 Loss 0.5328 Elapsed time 25.69s\n",
            "Epoch 7 Batch 600 Loss 0.4453 Elapsed time 25.75s\n",
            "Epoch 7 Batch 660 Loss 0.4948 Elapsed time 25.77s\n",
            "Epoch 7 Batch 720 Loss 0.3933 Elapsed time 25.73s\n",
            "Epoch 7 Batch 780 Loss 0.4851 Elapsed time 25.69s\n",
            "Epoch 7 Batch 840 Loss 0.4670 Elapsed time 25.68s\n",
            "Epoch 7 Batch 900 Loss 0.4434 Elapsed time 25.87s\n",
            "Белсә килер иде… Әнисеннән әтисе хакында берничә мәртәбә сорап та караган иде, әмма әнисе бу хакта сөйләргә яратмый .\n",
            "[[5075, 1174, 5967, 10419, 421, 676, 173, 488, 792, 63, 1021, 91, 82, 377, 8, 660, 930, 2753, 1]]\n",
            "хотелось бы поскорее бы поскорее не рассказать о несколько раз о аминэ . <end>\n",
            "Epoch 8 Batch 0 Loss 0.4725 Elapsed time 15.38s\n",
            "Epoch 8 Batch 60 Loss 0.4806 Elapsed time 25.73s\n",
            "Epoch 8 Batch 120 Loss 0.4574 Elapsed time 25.85s\n",
            "Epoch 8 Batch 180 Loss 0.4769 Elapsed time 25.77s\n",
            "Epoch 8 Batch 240 Loss 0.4297 Elapsed time 25.77s\n",
            "Epoch 8 Batch 300 Loss 0.4960 Elapsed time 25.82s\n",
            "Epoch 8 Batch 360 Loss 0.4386 Elapsed time 25.80s\n",
            "Epoch 8 Batch 420 Loss 0.4097 Elapsed time 25.68s\n",
            "Epoch 8 Batch 480 Loss 0.4933 Elapsed time 25.70s\n",
            "Epoch 8 Batch 540 Loss 0.5026 Elapsed time 25.79s\n",
            "Epoch 8 Batch 600 Loss 0.4157 Elapsed time 25.64s\n",
            "Epoch 8 Batch 660 Loss 0.4590 Elapsed time 25.67s\n",
            "Epoch 8 Batch 720 Loss 0.3697 Elapsed time 25.74s\n",
            "Epoch 8 Batch 780 Loss 0.4519 Elapsed time 25.78s\n",
            "Epoch 8 Batch 840 Loss 0.4395 Elapsed time 25.86s\n",
            "Epoch 8 Batch 900 Loss 0.4078 Elapsed time 25.81s\n",
            "Юк, гаиләдә бәхетле мин .\n",
            "[[55, 4630, 448, 10, 1]]\n",
            "нет, я был в семье . <end>\n",
            "Epoch 9 Batch 0 Loss 0.4360 Elapsed time 15.05s\n",
            "Epoch 9 Batch 60 Loss 0.4548 Elapsed time 25.70s\n",
            "Epoch 9 Batch 120 Loss 0.4352 Elapsed time 25.72s\n",
            "Epoch 9 Batch 180 Loss 0.4480 Elapsed time 25.72s\n",
            "Epoch 9 Batch 240 Loss 0.4032 Elapsed time 25.67s\n",
            "Epoch 9 Batch 300 Loss 0.4605 Elapsed time 25.77s\n",
            "Epoch 9 Batch 360 Loss 0.4108 Elapsed time 25.75s\n",
            "Epoch 9 Batch 420 Loss 0.3863 Elapsed time 25.70s\n",
            "Epoch 9 Batch 480 Loss 0.4577 Elapsed time 25.73s\n",
            "Epoch 9 Batch 540 Loss 0.4694 Elapsed time 25.73s\n",
            "Epoch 9 Batch 600 Loss 0.3881 Elapsed time 25.68s\n",
            "Epoch 9 Batch 660 Loss 0.4282 Elapsed time 25.73s\n",
            "Epoch 9 Batch 720 Loss 0.3526 Elapsed time 25.85s\n",
            "Epoch 9 Batch 780 Loss 0.4316 Elapsed time 25.63s\n",
            "Epoch 9 Batch 840 Loss 0.4077 Elapsed time 25.75s\n",
            "Epoch 9 Batch 900 Loss 0.3884 Elapsed time 25.76s\n",
            "Америкалылар, безгә гуманитар ярдәм йөзеннән, әлеге электрон почтаны эшләп бирделәр .\n",
            "[[55809, 167, 27925, 395, 21691, 256, 10094, 55810, 576, 1418, 1]]\n",
            "от того, чтобы не вызвали к себе права избавиться от того, чтобы вы избавиться\n",
            "Epoch 10 Batch 0 Loss 0.4153 Elapsed time 15.40s\n",
            "Epoch 10 Batch 60 Loss 0.4287 Elapsed time 25.72s\n",
            "Epoch 10 Batch 120 Loss 0.4111 Elapsed time 25.71s\n",
            "Epoch 10 Batch 180 Loss 0.4215 Elapsed time 25.71s\n",
            "Epoch 10 Batch 240 Loss 0.3762 Elapsed time 25.74s\n",
            "Epoch 10 Batch 300 Loss 0.4464 Elapsed time 25.69s\n",
            "Epoch 10 Batch 360 Loss 0.3743 Elapsed time 25.77s\n",
            "Epoch 10 Batch 420 Loss 0.3600 Elapsed time 25.69s\n",
            "Epoch 10 Batch 480 Loss 0.4294 Elapsed time 25.72s\n",
            "Epoch 10 Batch 540 Loss 0.4316 Elapsed time 25.68s\n",
            "Epoch 10 Batch 600 Loss 0.3752 Elapsed time 25.70s\n",
            "Epoch 10 Batch 660 Loss 0.4094 Elapsed time 25.63s\n",
            "Epoch 10 Batch 720 Loss 0.3192 Elapsed time 25.73s\n",
            "Epoch 10 Batch 780 Loss 0.4050 Elapsed time 25.72s\n",
            "Epoch 10 Batch 840 Loss 0.3868 Elapsed time 25.70s\n",
            "Epoch 10 Batch 900 Loss 0.3836 Elapsed time 25.67s\n",
            "Нәкъ шулчак звонок шалтырады .\n",
            "[[211, 557, 3612, 5569, 1]]\n",
            "в момент раздался звонок . <end>\n",
            "Epoch 11 Batch 0 Loss 0.3967 Elapsed time 14.91s\n",
            "Epoch 11 Batch 60 Loss 0.3945 Elapsed time 25.68s\n",
            "Epoch 11 Batch 120 Loss 0.3937 Elapsed time 25.80s\n",
            "Epoch 11 Batch 180 Loss 0.3974 Elapsed time 25.68s\n",
            "Epoch 11 Batch 240 Loss 0.3547 Elapsed time 25.82s\n",
            "Epoch 11 Batch 300 Loss 0.4245 Elapsed time 25.69s\n",
            "Epoch 11 Batch 360 Loss 0.3514 Elapsed time 25.73s\n",
            "Epoch 11 Batch 420 Loss 0.3376 Elapsed time 25.71s\n",
            "Epoch 11 Batch 480 Loss 0.4041 Elapsed time 25.67s\n",
            "Epoch 11 Batch 540 Loss 0.4158 Elapsed time 25.75s\n",
            "Epoch 11 Batch 600 Loss 0.3522 Elapsed time 25.70s\n",
            "Epoch 11 Batch 660 Loss 0.3993 Elapsed time 25.69s\n",
            "Epoch 11 Batch 720 Loss 0.3036 Elapsed time 25.71s\n",
            "Epoch 11 Batch 780 Loss 0.3757 Elapsed time 25.75s\n",
            "Epoch 11 Batch 840 Loss 0.3652 Elapsed time 25.68s\n",
            "Epoch 11 Batch 900 Loss 0.3449 Elapsed time 25.71s\n",
            "Кайчандыр Миләүшә мәктәбендә алар әүвәл Сабир белән бер партада утырып укыдылар .\n",
            "[[974, 2801, 7461, 38, 2494, 653, 6, 7, 26376, 622, 26377, 1]]\n",
            "когда-то в одной руке должен идти к матери, а к нему на одной руке\n",
            "Epoch 12 Batch 0 Loss 0.3616 Elapsed time 15.46s\n",
            "Epoch 12 Batch 60 Loss 0.3861 Elapsed time 25.75s\n",
            "Epoch 12 Batch 120 Loss 0.3795 Elapsed time 25.80s\n",
            "Epoch 12 Batch 180 Loss 0.3779 Elapsed time 25.83s\n",
            "Epoch 12 Batch 240 Loss 0.3312 Elapsed time 26.04s\n",
            "Epoch 12 Batch 300 Loss 0.4060 Elapsed time 26.16s\n",
            "Epoch 12 Batch 360 Loss 0.3290 Elapsed time 25.99s\n",
            "Epoch 12 Batch 420 Loss 0.3174 Elapsed time 25.84s\n",
            "Epoch 12 Batch 480 Loss 0.3827 Elapsed time 25.88s\n",
            "Epoch 12 Batch 540 Loss 0.4015 Elapsed time 25.86s\n",
            "Epoch 12 Batch 600 Loss 0.3267 Elapsed time 25.74s\n",
            "Epoch 12 Batch 660 Loss 0.3794 Elapsed time 25.70s\n",
            "Epoch 12 Batch 720 Loss 0.2997 Elapsed time 25.76s\n",
            "Epoch 12 Batch 780 Loss 0.3747 Elapsed time 25.74s\n",
            "Epoch 12 Batch 840 Loss 0.3562 Elapsed time 25.78s\n",
            "Epoch 12 Batch 900 Loss 0.3345 Elapsed time 25.74s\n",
            "Киресенчә, олы бер эштән, җаваплылыктан котылган кеше кебек дәртләнеп, көлеп-елмаеп йөри башлады .\n",
            "[[879, 357, 7, 17528, 16088, 16160, 25, 42, 11518, 20186, 188, 73, 1]]\n",
            "наоборот, он и смеялся, словно который начинает жить с тем, что он начинает ходить\n",
            "Epoch 13 Batch 0 Loss 0.3453 Elapsed time 15.41s\n",
            "Epoch 13 Batch 60 Loss 0.3660 Elapsed time 25.76s\n",
            "Epoch 13 Batch 120 Loss 0.3648 Elapsed time 25.70s\n",
            "Epoch 13 Batch 180 Loss 0.3596 Elapsed time 25.86s\n",
            "Epoch 13 Batch 240 Loss 0.3249 Elapsed time 26.31s\n",
            "Epoch 13 Batch 300 Loss 0.3848 Elapsed time 26.23s\n",
            "Epoch 13 Batch 360 Loss 0.3243 Elapsed time 26.00s\n",
            "Epoch 13 Batch 420 Loss 0.3174 Elapsed time 25.80s\n",
            "Epoch 13 Batch 480 Loss 0.3816 Elapsed time 25.98s\n",
            "Epoch 13 Batch 540 Loss 0.3966 Elapsed time 25.90s\n",
            "Epoch 13 Batch 600 Loss 0.3224 Elapsed time 25.70s\n",
            "Epoch 13 Batch 660 Loss 0.3568 Elapsed time 25.77s\n",
            "Epoch 13 Batch 720 Loss 0.2911 Elapsed time 25.87s\n",
            "Epoch 13 Batch 780 Loss 0.3494 Elapsed time 25.65s\n",
            "Epoch 13 Batch 840 Loss 0.3524 Elapsed time 25.79s\n",
            "Epoch 13 Batch 900 Loss 0.3275 Elapsed time 25.79s\n",
            "Менә шулай итеп, мәсьәләләр чишә-чишә ярты төн узды .\n",
            "[[23, 33, 277, 7476, 39876, 514, 411, 827, 1]]\n",
            "вот так и прошла прошла прошла прошла прошла прошла прошла прошла ночь . <end>\n",
            "Epoch 14 Batch 0 Loss 0.3412 Elapsed time 15.32s\n",
            "Epoch 14 Batch 60 Loss 0.3530 Elapsed time 25.85s\n",
            "Epoch 14 Batch 120 Loss 0.3644 Elapsed time 25.79s\n",
            "Epoch 14 Batch 180 Loss 0.3526 Elapsed time 25.74s\n",
            "Epoch 14 Batch 240 Loss 0.3102 Elapsed time 25.72s\n",
            "Epoch 14 Batch 300 Loss 0.3739 Elapsed time 25.89s\n",
            "Epoch 14 Batch 360 Loss 0.3060 Elapsed time 25.86s\n",
            "Epoch 14 Batch 420 Loss 0.2985 Elapsed time 25.64s\n",
            "Epoch 14 Batch 480 Loss 0.3773 Elapsed time 25.87s\n",
            "Epoch 14 Batch 540 Loss 0.3905 Elapsed time 25.66s\n",
            "Epoch 14 Batch 600 Loss 0.2901 Elapsed time 25.70s\n",
            "Epoch 14 Batch 660 Loss 0.3521 Elapsed time 25.68s\n",
            "Epoch 14 Batch 720 Loss 0.2838 Elapsed time 25.69s\n",
            "Epoch 14 Batch 780 Loss 0.3404 Elapsed time 25.78s\n",
            "Epoch 14 Batch 840 Loss 0.3437 Elapsed time 25.69s\n",
            "Epoch 14 Batch 900 Loss 0.3136 Elapsed time 25.78s\n",
            "Ундүрт яшендә диген бит әле !\n",
            "[[7889, 13503, 2672, 22, 19, 9]]\n",
            "скажи, что в состоянии ! <end>\n",
            "Epoch 15 Batch 0 Loss 0.3358 Elapsed time 15.25s\n",
            "Epoch 15 Batch 60 Loss 0.3485 Elapsed time 26.32s\n",
            "Epoch 15 Batch 120 Loss 0.3486 Elapsed time 26.35s\n",
            "Epoch 15 Batch 180 Loss 0.3352 Elapsed time 25.94s\n",
            "Epoch 15 Batch 240 Loss 0.3076 Elapsed time 25.94s\n",
            "Epoch 15 Batch 300 Loss 0.3547 Elapsed time 25.89s\n",
            "Epoch 15 Batch 360 Loss 0.2998 Elapsed time 25.93s\n",
            "Epoch 15 Batch 420 Loss 0.2907 Elapsed time 25.87s\n",
            "Epoch 15 Batch 480 Loss 0.3498 Elapsed time 25.74s\n",
            "Epoch 15 Batch 540 Loss 0.3700 Elapsed time 25.87s\n",
            "Epoch 15 Batch 600 Loss 0.2904 Elapsed time 25.83s\n",
            "Epoch 15 Batch 660 Loss 0.3574 Elapsed time 25.84s\n",
            "Epoch 15 Batch 720 Loss 0.2621 Elapsed time 25.83s\n",
            "Epoch 15 Batch 780 Loss 0.3376 Elapsed time 25.92s\n",
            "Epoch 15 Batch 840 Loss 0.3109 Elapsed time 25.90s\n",
            "Epoch 15 Batch 900 Loss 0.2987 Elapsed time 26.05s\n",
            "Бүген Зиннуров хастаханәгә иртүк килде .\n",
            "[[80, 1499, 993, 4020, 116, 1]]\n",
            "сегодня рано утром зиннуров пришел рано утром . <end>\n",
            "Epoch 16 Batch 0 Loss 0.3296 Elapsed time 15.12s\n",
            "Epoch 16 Batch 60 Loss 0.3419 Elapsed time 25.73s\n",
            "Epoch 16 Batch 120 Loss 0.3222 Elapsed time 25.85s\n",
            "Epoch 16 Batch 180 Loss 0.3208 Elapsed time 25.76s\n",
            "Epoch 16 Batch 240 Loss 0.2896 Elapsed time 25.75s\n",
            "Epoch 16 Batch 300 Loss 0.3417 Elapsed time 25.82s\n",
            "Epoch 16 Batch 360 Loss 0.2986 Elapsed time 25.76s\n",
            "Epoch 16 Batch 420 Loss 0.2756 Elapsed time 25.89s\n",
            "Epoch 16 Batch 480 Loss 0.3497 Elapsed time 25.74s\n",
            "Epoch 16 Batch 540 Loss 0.3501 Elapsed time 25.85s\n",
            "Epoch 16 Batch 600 Loss 0.2896 Elapsed time 25.99s\n",
            "Epoch 16 Batch 660 Loss 0.3433 Elapsed time 25.86s\n",
            "Epoch 16 Batch 720 Loss 0.2532 Elapsed time 25.75s\n",
            "Epoch 16 Batch 780 Loss 0.3268 Elapsed time 25.65s\n",
            "Epoch 16 Batch 840 Loss 0.3092 Elapsed time 25.70s\n",
            "Epoch 16 Batch 900 Loss 0.2963 Elapsed time 25.76s\n",
            "Бабакай, мин рояль уйныйм, Ләйлә апа сезгә җырлап күрсәтер,диде дә рояль янына барып утырды .\n",
            "[[27458, 10, 3967, 27460, 3909, 430, 205, 1896, 24857, 5, 3967, 69, 133, 240, 1]]\n",
            "дедушка, дедушка, я позвал вам сказал, что вернусь к вам и сел за луг\n",
            "Epoch 17 Batch 0 Loss 0.3229 Elapsed time 15.30s\n",
            "Epoch 17 Batch 60 Loss 0.3341 Elapsed time 26.40s\n",
            "Epoch 17 Batch 120 Loss 0.3374 Elapsed time 26.74s\n",
            "Epoch 17 Batch 180 Loss 0.3254 Elapsed time 26.43s\n",
            "Epoch 17 Batch 240 Loss 0.2761 Elapsed time 26.34s\n",
            "Epoch 17 Batch 300 Loss 0.3299 Elapsed time 26.59s\n",
            "Epoch 17 Batch 360 Loss 0.2739 Elapsed time 26.38s\n",
            "Epoch 17 Batch 420 Loss 0.2625 Elapsed time 26.61s\n",
            "Epoch 17 Batch 480 Loss 0.3302 Elapsed time 26.49s\n",
            "Epoch 17 Batch 540 Loss 0.3454 Elapsed time 26.50s\n",
            "Epoch 17 Batch 600 Loss 0.2812 Elapsed time 26.56s\n",
            "Epoch 17 Batch 660 Loss 0.3309 Elapsed time 26.81s\n",
            "Epoch 17 Batch 720 Loss 0.2482 Elapsed time 26.64s\n",
            "Epoch 17 Batch 780 Loss 0.3098 Elapsed time 26.73s\n",
            "Epoch 17 Batch 840 Loss 0.3017 Elapsed time 26.73s\n",
            "Epoch 17 Batch 900 Loss 0.2782 Elapsed time 26.77s\n",
            "Улының бүген үзен бик тә бирелеп тыңлавы аның күңелен тәмам йомшаткан иде .\n",
            "[[5032, 80, 234, 16, 45, 3009, 21454, 11, 569, 335, 62723, 4, 1]]\n",
            "сегодня его сердце его окончательно была его мама . <end>\n",
            "Epoch 18 Batch 0 Loss 0.3173 Elapsed time 15.77s\n",
            "Epoch 18 Batch 60 Loss 0.3269 Elapsed time 26.41s\n",
            "Epoch 18 Batch 120 Loss 0.3184 Elapsed time 26.42s\n",
            "Epoch 18 Batch 180 Loss 0.3094 Elapsed time 26.84s\n",
            "Epoch 18 Batch 240 Loss 0.2771 Elapsed time 26.59s\n",
            "Epoch 18 Batch 300 Loss 0.3244 Elapsed time 26.65s\n",
            "Epoch 18 Batch 360 Loss 0.2796 Elapsed time 26.69s\n",
            "Epoch 18 Batch 420 Loss 0.2586 Elapsed time 26.75s\n",
            "Epoch 18 Batch 480 Loss 0.3269 Elapsed time 26.79s\n",
            "Epoch 18 Batch 540 Loss 0.3356 Elapsed time 26.57s\n",
            "Epoch 18 Batch 600 Loss 0.2843 Elapsed time 26.71s\n",
            "Epoch 18 Batch 660 Loss 0.3350 Elapsed time 26.75s\n",
            "Epoch 18 Batch 720 Loss 0.2378 Elapsed time 26.78s\n",
            "Epoch 18 Batch 780 Loss 0.2962 Elapsed time 26.74s\n",
            "Epoch 18 Batch 840 Loss 0.2957 Elapsed time 26.98s\n",
            "Epoch 18 Batch 900 Loss 0.2765 Elapsed time 26.73s\n",
            "Әмма шунысы бар болай эшләү эчтән авыр, эчтән кыен иде Таһирга .\n",
            "[[82, 2643, 30, 131, 2648, 1066, 1888, 1066, 665, 4, 1626, 1]]\n",
            "но тагиру так трудно понять, что это понять, что очень приятно . <end>\n",
            "Epoch 19 Batch 0 Loss 0.3018 Elapsed time 16.15s\n",
            "Epoch 19 Batch 60 Loss 0.3205 Elapsed time 26.82s\n",
            "Epoch 19 Batch 120 Loss 0.3128 Elapsed time 26.81s\n",
            "Epoch 19 Batch 180 Loss 0.3111 Elapsed time 26.76s\n",
            "Epoch 19 Batch 240 Loss 0.2784 Elapsed time 26.81s\n",
            "Epoch 19 Batch 300 Loss 0.3140 Elapsed time 26.71s\n",
            "Epoch 19 Batch 360 Loss 0.2677 Elapsed time 26.37s\n",
            "Epoch 19 Batch 420 Loss 0.2489 Elapsed time 26.32s\n",
            "Epoch 19 Batch 480 Loss 0.3128 Elapsed time 26.57s\n",
            "Epoch 19 Batch 540 Loss 0.3352 Elapsed time 26.44s\n",
            "Epoch 19 Batch 600 Loss 0.2751 Elapsed time 26.64s\n",
            "Epoch 19 Batch 660 Loss 0.3256 Elapsed time 26.66s\n",
            "Epoch 19 Batch 720 Loss 0.2492 Elapsed time 26.80s\n",
            "Epoch 19 Batch 780 Loss 0.2872 Elapsed time 26.34s\n",
            "Epoch 19 Batch 840 Loss 0.2963 Elapsed time 26.45s\n",
            "Epoch 19 Batch 900 Loss 0.2705 Elapsed time 26.44s\n",
            "Миңа калса, эш башкада иде . . .\n",
            "[[58, 1204, 86, 23204, 4, 1, 1, 1]]\n",
            "мне кажется, дело в другом . . . <end>\n",
            "Epoch 20 Batch 0 Loss 0.3036 Elapsed time 15.56s\n",
            "Epoch 20 Batch 60 Loss 0.3176 Elapsed time 26.33s\n",
            "Epoch 20 Batch 120 Loss 0.3033 Elapsed time 26.34s\n",
            "Epoch 20 Batch 180 Loss 0.3014 Elapsed time 26.24s\n",
            "Epoch 20 Batch 240 Loss 0.2646 Elapsed time 26.66s\n",
            "Epoch 20 Batch 300 Loss 0.3118 Elapsed time 26.82s\n",
            "Epoch 20 Batch 360 Loss 0.2669 Elapsed time 26.83s\n",
            "Epoch 20 Batch 420 Loss 0.2467 Elapsed time 26.80s\n",
            "Epoch 20 Batch 480 Loss 0.3101 Elapsed time 26.95s\n",
            "Epoch 20 Batch 540 Loss 0.3291 Elapsed time 26.89s\n",
            "Epoch 20 Batch 600 Loss 0.2765 Elapsed time 26.85s\n",
            "Epoch 20 Batch 660 Loss 0.3234 Elapsed time 26.85s\n",
            "Epoch 20 Batch 720 Loss 0.2487 Elapsed time 26.96s\n",
            "Epoch 20 Batch 780 Loss 0.2891 Elapsed time 26.65s\n",
            "Epoch 20 Batch 840 Loss 0.2866 Elapsed time 26.20s\n",
            "Epoch 20 Batch 900 Loss 0.2682 Elapsed time 26.10s\n",
            "Беләсеңме, без әтине бик олылап искә алабыз бит .\n",
            "[[3187, 49, 7005, 16, 21549, 425, 3285, 22, 1]]\n",
            "знаешь, с большим криком . <end>\n",
            "Epoch 21 Batch 0 Loss 0.2885 Elapsed time 15.11s\n",
            "Epoch 21 Batch 60 Loss 0.3076 Elapsed time 26.04s\n",
            "Epoch 21 Batch 120 Loss 0.3001 Elapsed time 26.19s\n",
            "Epoch 21 Batch 180 Loss 0.2948 Elapsed time 26.44s\n",
            "Epoch 21 Batch 240 Loss 0.2590 Elapsed time 26.50s\n",
            "Epoch 21 Batch 300 Loss 0.3086 Elapsed time 26.33s\n",
            "Epoch 21 Batch 360 Loss 0.2567 Elapsed time 26.21s\n",
            "Epoch 21 Batch 420 Loss 0.2322 Elapsed time 26.69s\n",
            "Epoch 21 Batch 480 Loss 0.3074 Elapsed time 26.45s\n",
            "Epoch 21 Batch 540 Loss 0.3248 Elapsed time 26.81s\n",
            "Epoch 21 Batch 600 Loss 0.2641 Elapsed time 26.80s\n",
            "Epoch 21 Batch 660 Loss 0.3020 Elapsed time 26.83s\n",
            "Epoch 21 Batch 720 Loss 0.2369 Elapsed time 26.87s\n",
            "Epoch 21 Batch 780 Loss 0.2820 Elapsed time 26.88s\n",
            "Epoch 21 Batch 840 Loss 0.2828 Elapsed time 26.81s\n",
            "Epoch 21 Batch 900 Loss 0.2548 Elapsed time 26.67s\n",
            "Үзем дә рецензия турында кичә генә белдем . . .\n",
            "[[165, 5, 29497, 57, 459, 13, 4096, 1, 1, 1]]\n",
            "я вчера перед ними узнал о да и раньше . . . <end>\n",
            "Epoch 22 Batch 0 Loss 0.2878 Elapsed time 15.99s\n",
            "Epoch 22 Batch 60 Loss 0.2909 Elapsed time 26.71s\n",
            "Epoch 22 Batch 120 Loss 0.2882 Elapsed time 26.59s\n",
            "Epoch 22 Batch 180 Loss 0.2945 Elapsed time 26.65s\n",
            "Epoch 22 Batch 240 Loss 0.2456 Elapsed time 26.71s\n",
            "Epoch 22 Batch 300 Loss 0.2995 Elapsed time 26.65s\n",
            "Epoch 22 Batch 360 Loss 0.2434 Elapsed time 26.83s\n",
            "Epoch 22 Batch 420 Loss 0.2398 Elapsed time 26.38s\n",
            "Epoch 22 Batch 480 Loss 0.3074 Elapsed time 26.62s\n",
            "Epoch 22 Batch 540 Loss 0.3153 Elapsed time 26.70s\n",
            "Epoch 22 Batch 600 Loss 0.2489 Elapsed time 26.71s\n",
            "Epoch 22 Batch 660 Loss 0.3110 Elapsed time 26.67s\n",
            "Epoch 22 Batch 720 Loss 0.2314 Elapsed time 26.59s\n",
            "Epoch 22 Batch 780 Loss 0.2720 Elapsed time 26.70s\n",
            "Epoch 22 Batch 840 Loss 0.2792 Elapsed time 26.78s\n",
            "Epoch 22 Batch 900 Loss 0.2429 Elapsed time 26.59s\n",
            "әни, аклангандай итеп, авыр сулап куйды .\n",
            "[[2545, 17634, 277, 182, 1205, 83, 1]]\n",
            "мама, вздохнул тяжело вздохнул . <end>\n",
            "Epoch 23 Batch 0 Loss 0.2841 Elapsed time 15.07s\n",
            "Epoch 23 Batch 60 Loss 0.3006 Elapsed time 26.23s\n",
            "Epoch 23 Batch 120 Loss 0.2869 Elapsed time 26.12s\n",
            "Epoch 23 Batch 180 Loss 0.2892 Elapsed time 26.16s\n",
            "Epoch 23 Batch 240 Loss 0.2492 Elapsed time 26.80s\n",
            "Epoch 23 Batch 300 Loss 0.2952 Elapsed time 26.50s\n",
            "Epoch 23 Batch 360 Loss 0.2377 Elapsed time 26.73s\n",
            "Epoch 23 Batch 420 Loss 0.2282 Elapsed time 26.66s\n",
            "Epoch 23 Batch 480 Loss 0.2914 Elapsed time 26.63s\n",
            "Epoch 23 Batch 540 Loss 0.3162 Elapsed time 26.68s\n",
            "Epoch 23 Batch 600 Loss 0.2537 Elapsed time 26.56s\n",
            "Epoch 23 Batch 660 Loss 0.3027 Elapsed time 26.65s\n",
            "Epoch 23 Batch 720 Loss 0.2311 Elapsed time 26.64s\n",
            "Epoch 23 Batch 780 Loss 0.2782 Elapsed time 26.70s\n",
            "Epoch 23 Batch 840 Loss 0.2709 Elapsed time 26.71s\n",
            "Epoch 23 Batch 900 Loss 0.2421 Elapsed time 26.68s\n",
            "Менә кичә туй балдагымны сатып акча юнәттем .\n",
            "[[23, 459, 1170, 52233, 1035, 270, 52234, 1]]\n",
            "вчера путь к теперь улицу упал . <end>\n",
            "Epoch 24 Batch 0 Loss 0.2747 Elapsed time 15.72s\n",
            "Epoch 24 Batch 60 Loss 0.2998 Elapsed time 26.51s\n",
            "Epoch 24 Batch 120 Loss 0.2832 Elapsed time 26.40s\n",
            "Epoch 24 Batch 180 Loss 0.2812 Elapsed time 26.31s\n",
            "Epoch 24 Batch 240 Loss 0.2407 Elapsed time 26.24s\n",
            "Epoch 24 Batch 300 Loss 0.2914 Elapsed time 26.36s\n",
            "Epoch 24 Batch 360 Loss 0.2394 Elapsed time 26.32s\n",
            "Epoch 24 Batch 420 Loss 0.2158 Elapsed time 26.33s\n",
            "Epoch 24 Batch 480 Loss 0.2878 Elapsed time 26.04s\n",
            "Epoch 24 Batch 540 Loss 0.3106 Elapsed time 26.07s\n",
            "Epoch 24 Batch 600 Loss 0.2412 Elapsed time 26.06s\n",
            "Epoch 24 Batch 660 Loss 0.3015 Elapsed time 26.10s\n",
            "Epoch 24 Batch 720 Loss 0.2213 Elapsed time 26.09s\n",
            "Epoch 24 Batch 780 Loss 0.2637 Elapsed time 26.05s\n",
            "Epoch 24 Batch 840 Loss 0.2687 Elapsed time 26.05s\n",
            "Epoch 24 Batch 900 Loss 0.2345 Elapsed time 25.96s\n",
            "Әхсән, алдан сөйләшеп куелганча, кичәне башлап җи­бәрер өчен берничә генә сүз әйтеп китәргә тиеш иде .\n",
            "[[2927, 687, 413, 45123, 8786, 1051, 45124, 28, 173, 13, 67, 243, 386, 113, 4, 1]]\n",
            "ахтэм мог разобраться уже тяжелее . <end>\n",
            "Epoch 25 Batch 0 Loss 0.2636 Elapsed time 15.08s\n",
            "Epoch 25 Batch 60 Loss 0.2965 Elapsed time 25.97s\n",
            "Epoch 25 Batch 120 Loss 0.2793 Elapsed time 25.90s\n",
            "Epoch 25 Batch 180 Loss 0.2824 Elapsed time 25.92s\n",
            "Epoch 25 Batch 240 Loss 0.2374 Elapsed time 25.84s\n",
            "Epoch 25 Batch 300 Loss 0.2910 Elapsed time 25.97s\n",
            "Epoch 25 Batch 360 Loss 0.2366 Elapsed time 25.81s\n",
            "Epoch 25 Batch 420 Loss 0.2231 Elapsed time 25.89s\n",
            "Epoch 25 Batch 480 Loss 0.2827 Elapsed time 25.89s\n",
            "Epoch 25 Batch 540 Loss 0.3034 Elapsed time 25.71s\n",
            "Epoch 25 Batch 600 Loss 0.2499 Elapsed time 25.75s\n",
            "Epoch 25 Batch 660 Loss 0.2969 Elapsed time 25.86s\n",
            "Epoch 25 Batch 720 Loss 0.2195 Elapsed time 25.83s\n",
            "Epoch 25 Batch 780 Loss 0.2589 Elapsed time 25.76s\n",
            "Epoch 25 Batch 840 Loss 0.2623 Elapsed time 25.87s\n",
            "Epoch 25 Batch 900 Loss 0.2302 Elapsed time 25.74s\n",
            "Ләкин мине фронтка, частема җибәрегез,диде .\n",
            "[[27, 95, 30559, 30560, 30561, 1]]\n",
            "но он сказал, что должен был надеть на меня тахир или что на этом\n",
            "Epoch 26 Batch 0 Loss 0.2679 Elapsed time 15.32s\n",
            "Epoch 26 Batch 60 Loss 0.2900 Elapsed time 25.70s\n",
            "Epoch 26 Batch 120 Loss 0.2747 Elapsed time 25.76s\n",
            "Epoch 26 Batch 180 Loss 0.2759 Elapsed time 25.65s\n",
            "Epoch 26 Batch 240 Loss 0.2269 Elapsed time 25.82s\n",
            "Epoch 26 Batch 300 Loss 0.2745 Elapsed time 25.77s\n",
            "Epoch 26 Batch 360 Loss 0.2298 Elapsed time 25.79s\n",
            "Epoch 26 Batch 420 Loss 0.2155 Elapsed time 25.96s\n",
            "Epoch 26 Batch 480 Loss 0.2869 Elapsed time 25.96s\n",
            "Epoch 26 Batch 540 Loss 0.2957 Elapsed time 25.92s\n",
            "Epoch 26 Batch 600 Loss 0.2367 Elapsed time 25.77s\n",
            "Epoch 26 Batch 660 Loss 0.2928 Elapsed time 26.09s\n",
            "Epoch 26 Batch 720 Loss 0.2181 Elapsed time 26.97s\n",
            "Epoch 26 Batch 780 Loss 0.2600 Elapsed time 26.51s\n",
            "Epoch 26 Batch 840 Loss 0.2533 Elapsed time 26.37s\n",
            "Epoch 26 Batch 900 Loss 0.2274 Elapsed time 26.71s\n",
            "Лагерь йокыга талган иде .\n",
            "[[3497, 1312, 9503, 4, 1]]\n",
            "лежал навстречу . <end>\n",
            "Epoch 27 Batch 0 Loss 0.2557 Elapsed time 15.28s\n",
            "Epoch 27 Batch 60 Loss 0.2849 Elapsed time 26.51s\n",
            "Epoch 27 Batch 120 Loss 0.2647 Elapsed time 26.48s\n",
            "Epoch 27 Batch 180 Loss 0.2673 Elapsed time 26.46s\n",
            "Epoch 27 Batch 240 Loss 0.2299 Elapsed time 26.47s\n",
            "Epoch 27 Batch 300 Loss 0.2736 Elapsed time 26.46s\n",
            "Epoch 27 Batch 360 Loss 0.2286 Elapsed time 26.79s\n",
            "Epoch 27 Batch 420 Loss 0.2079 Elapsed time 26.62s\n",
            "Epoch 27 Batch 480 Loss 0.2786 Elapsed time 26.82s\n",
            "Epoch 27 Batch 540 Loss 0.2851 Elapsed time 26.67s\n",
            "Epoch 27 Batch 600 Loss 0.2357 Elapsed time 26.69s\n",
            "Epoch 27 Batch 660 Loss 0.2921 Elapsed time 26.66s\n",
            "Epoch 27 Batch 720 Loss 0.2122 Elapsed time 26.58s\n",
            "Epoch 27 Batch 780 Loss 0.2542 Elapsed time 26.66s\n",
            "Epoch 27 Batch 840 Loss 0.2525 Elapsed time 26.29s\n",
            "Epoch 27 Batch 900 Loss 0.2321 Elapsed time 26.57s\n",
            "Үзенең уйлап табулары белән заводка берничә мең сумнар экономия бирде .\n",
            "[[89, 263, 26916, 6, 2687, 173, 400, 26917, 10307, 261, 1]]\n",
            "свое десять передали друга к завод, прежде всего передали неделю . <end>\n",
            "Epoch 28 Batch 0 Loss 0.2603 Elapsed time 15.79s\n",
            "Epoch 28 Batch 60 Loss 0.2811 Elapsed time 26.62s\n",
            "Epoch 28 Batch 120 Loss 0.2697 Elapsed time 26.66s\n",
            "Epoch 28 Batch 180 Loss 0.2655 Elapsed time 26.58s\n",
            "Epoch 28 Batch 240 Loss 0.2271 Elapsed time 26.18s\n",
            "Epoch 28 Batch 300 Loss 0.2622 Elapsed time 26.11s\n",
            "Epoch 28 Batch 360 Loss 0.2205 Elapsed time 26.20s\n",
            "Epoch 28 Batch 420 Loss 0.2026 Elapsed time 26.26s\n",
            "Epoch 28 Batch 480 Loss 0.2736 Elapsed time 26.24s\n",
            "Epoch 28 Batch 540 Loss 0.2949 Elapsed time 26.25s\n",
            "Epoch 28 Batch 600 Loss 0.2370 Elapsed time 26.37s\n",
            "Epoch 28 Batch 660 Loss 0.2842 Elapsed time 26.75s\n",
            "Epoch 28 Batch 720 Loss 0.2043 Elapsed time 26.31s\n",
            "Epoch 28 Batch 780 Loss 0.2445 Elapsed time 26.65s\n",
            "Epoch 28 Batch 840 Loss 0.2503 Elapsed time 26.56s\n",
            "Epoch 28 Batch 900 Loss 0.2191 Elapsed time 26.71s\n",
            "Хәтәр шартлауга кадәр язып җибәргән хатын да хәтерли Булат .\n",
            "[[1867, 19791, 103, 693, 1318, 365, 2, 4717, 627, 1]]\n",
            "булат старательно успел довести до рынка . <end>\n",
            "Epoch 29 Batch 0 Loss 0.2495 Elapsed time 15.58s\n",
            "Epoch 29 Batch 60 Loss 0.2727 Elapsed time 26.57s\n",
            "Epoch 29 Batch 120 Loss 0.2657 Elapsed time 26.66s\n",
            "Epoch 29 Batch 180 Loss 0.2627 Elapsed time 26.51s\n",
            "Epoch 29 Batch 240 Loss 0.2219 Elapsed time 26.49s\n",
            "Epoch 29 Batch 300 Loss 0.2655 Elapsed time 26.19s\n",
            "Epoch 29 Batch 360 Loss 0.2208 Elapsed time 26.37s\n",
            "Epoch 29 Batch 420 Loss 0.1980 Elapsed time 26.30s\n",
            "Epoch 29 Batch 480 Loss 0.2692 Elapsed time 26.16s\n",
            "Epoch 29 Batch 540 Loss 0.2886 Elapsed time 26.31s\n",
            "Epoch 29 Batch 600 Loss 0.2292 Elapsed time 26.75s\n",
            "Epoch 29 Batch 660 Loss 0.2806 Elapsed time 26.51s\n",
            "Epoch 29 Batch 720 Loss 0.2110 Elapsed time 26.71s\n",
            "Epoch 29 Batch 780 Loss 0.2467 Elapsed time 26.71s\n",
            "Epoch 29 Batch 840 Loss 0.2548 Elapsed time 26.64s\n",
            "Epoch 29 Batch 900 Loss 0.2155 Elapsed time 26.72s\n",
            "Аның матур йөзенә алсулык йөгерде .\n",
            "[[11, 227, 473, 3407, 1185, 1]]\n",
            "его красивое лицо сияло роза . <end>\n",
            "Epoch 30 Batch 0 Loss 0.2468 Elapsed time 15.61s\n",
            "Epoch 30 Batch 60 Loss 0.2698 Elapsed time 26.61s\n",
            "Epoch 30 Batch 120 Loss 0.2555 Elapsed time 26.72s\n",
            "Epoch 30 Batch 180 Loss 0.2551 Elapsed time 26.75s\n",
            "Epoch 30 Batch 240 Loss 0.2232 Elapsed time 26.43s\n",
            "Epoch 30 Batch 300 Loss 0.2551 Elapsed time 26.51s\n",
            "Epoch 30 Batch 360 Loss 0.2118 Elapsed time 26.64s\n",
            "Epoch 30 Batch 420 Loss 0.2062 Elapsed time 26.66s\n",
            "Epoch 30 Batch 480 Loss 0.2630 Elapsed time 26.62s\n",
            "Epoch 30 Batch 540 Loss 0.2841 Elapsed time 26.73s\n",
            "Epoch 30 Batch 600 Loss 0.2344 Elapsed time 26.34s\n",
            "Epoch 30 Batch 660 Loss 0.2797 Elapsed time 26.41s\n",
            "Epoch 30 Batch 720 Loss 0.2018 Elapsed time 26.41s\n",
            "Epoch 30 Batch 780 Loss 0.2429 Elapsed time 26.73s\n",
            "Epoch 30 Batch 840 Loss 0.2441 Elapsed time 26.42s\n",
            "Epoch 30 Batch 900 Loss 0.2136 Elapsed time 26.58s\n",
            "Проводница купесы яныннан узганда, Мотыйк тагын кесәсеннән акча чыгарды .\n",
            "[[5449, 59312, 956, 19255, 1288, 40, 977, 270, 715, 1]]\n",
            "проходя руки мотык однако вынул деньги . <end>\n",
            "Epoch 31 Batch 0 Loss 0.2424 Elapsed time 15.67s\n",
            "Epoch 31 Batch 60 Loss 0.2668 Elapsed time 26.81s\n",
            "Epoch 31 Batch 120 Loss 0.2501 Elapsed time 26.75s\n",
            "Epoch 31 Batch 180 Loss 0.2646 Elapsed time 26.82s\n",
            "Epoch 31 Batch 240 Loss 0.2131 Elapsed time 26.87s\n",
            "Epoch 31 Batch 300 Loss 0.2546 Elapsed time 26.88s\n",
            "Epoch 31 Batch 360 Loss 0.2104 Elapsed time 26.84s\n",
            "Epoch 31 Batch 420 Loss 0.1980 Elapsed time 26.74s\n",
            "Epoch 31 Batch 480 Loss 0.2644 Elapsed time 26.78s\n",
            "Epoch 31 Batch 540 Loss 0.2753 Elapsed time 26.79s\n",
            "Epoch 31 Batch 600 Loss 0.2243 Elapsed time 26.55s\n",
            "Epoch 31 Batch 660 Loss 0.2635 Elapsed time 26.07s\n",
            "Epoch 31 Batch 720 Loss 0.1988 Elapsed time 26.11s\n",
            "Epoch 31 Batch 780 Loss 0.2352 Elapsed time 25.99s\n",
            "Epoch 31 Batch 840 Loss 0.2376 Elapsed time 26.12s\n",
            "Epoch 31 Batch 900 Loss 0.2114 Elapsed time 26.11s\n",
            "Нияте зурдан бит картның .\n",
            "[[6791, 5188, 22, 1106, 1]]\n",
            "ведь большие людей . <end>\n",
            "Epoch 32 Batch 0 Loss 0.2393 Elapsed time 15.31s\n",
            "Epoch 32 Batch 60 Loss 0.2603 Elapsed time 26.32s\n",
            "Epoch 32 Batch 120 Loss 0.2537 Elapsed time 26.19s\n",
            "Epoch 32 Batch 180 Loss 0.2534 Elapsed time 26.28s\n",
            "Epoch 32 Batch 240 Loss 0.2184 Elapsed time 26.64s\n",
            "Epoch 32 Batch 300 Loss 0.2526 Elapsed time 26.29s\n",
            "Epoch 32 Batch 360 Loss 0.2090 Elapsed time 26.63s\n",
            "Epoch 32 Batch 420 Loss 0.1903 Elapsed time 26.88s\n",
            "Epoch 32 Batch 480 Loss 0.2553 Elapsed time 26.73s\n",
            "Epoch 32 Batch 540 Loss 0.2634 Elapsed time 26.66s\n",
            "Epoch 32 Batch 600 Loss 0.2272 Elapsed time 26.74s\n",
            "Epoch 32 Batch 660 Loss 0.2721 Elapsed time 26.67s\n",
            "Epoch 32 Batch 720 Loss 0.1978 Elapsed time 26.54s\n",
            "Epoch 32 Batch 780 Loss 0.2253 Elapsed time 26.69s\n",
            "Epoch 32 Batch 840 Loss 0.2335 Elapsed time 26.38s\n",
            "Epoch 32 Batch 900 Loss 0.2023 Elapsed time 26.56s\n",
            "Лизаның соңгы ми­нутларын күрә алмады, янында була алмады Хәбир .\n",
            "[[4396, 174, 44269, 136, 1367, 251, 92, 302, 236, 1]]\n",
            "хабир не смог разглядеть последние взгляды лизы . <end>\n",
            "Epoch 33 Batch 0 Loss 0.2380 Elapsed time 15.63s\n",
            "Epoch 33 Batch 60 Loss 0.2581 Elapsed time 26.67s\n",
            "Epoch 33 Batch 120 Loss 0.2524 Elapsed time 26.67s\n",
            "Epoch 33 Batch 180 Loss 0.2476 Elapsed time 26.56s\n",
            "Epoch 33 Batch 240 Loss 0.2125 Elapsed time 26.67s\n",
            "Epoch 33 Batch 300 Loss 0.2460 Elapsed time 26.74s\n",
            "Epoch 33 Batch 360 Loss 0.2105 Elapsed time 26.67s\n",
            "Epoch 33 Batch 420 Loss 0.1897 Elapsed time 26.67s\n",
            "Epoch 33 Batch 480 Loss 0.2564 Elapsed time 26.65s\n",
            "Epoch 33 Batch 540 Loss 0.2775 Elapsed time 26.72s\n",
            "Epoch 33 Batch 600 Loss 0.2197 Elapsed time 26.84s\n",
            "Epoch 33 Batch 660 Loss 0.2708 Elapsed time 26.81s\n",
            "Epoch 33 Batch 720 Loss 0.1894 Elapsed time 26.78s\n",
            "Epoch 33 Batch 780 Loss 0.2212 Elapsed time 26.74s\n",
            "Epoch 33 Batch 840 Loss 0.2320 Elapsed time 26.84s\n",
            "Epoch 33 Batch 900 Loss 0.1963 Elapsed time 26.71s\n",
            "Ике ат җигеп кырдан киндер та­шыган чагыңны хәтерлим әле .\n",
            "[[54, 788, 17782, 43950, 8635, 43951, 25290, 8403, 19, 1]]\n",
            "помню, где ты на середине стройки не приехали тяжелый угол дороги . <end>\n",
            "Epoch 34 Batch 0 Loss 0.2301 Elapsed time 15.98s\n",
            "Epoch 34 Batch 60 Loss 0.2563 Elapsed time 26.65s\n",
            "Epoch 34 Batch 120 Loss 0.2459 Elapsed time 26.65s\n",
            "Epoch 34 Batch 180 Loss 0.2464 Elapsed time 26.68s\n",
            "Epoch 34 Batch 240 Loss 0.2094 Elapsed time 26.46s\n",
            "Epoch 34 Batch 300 Loss 0.2424 Elapsed time 26.30s\n",
            "Epoch 34 Batch 360 Loss 0.2059 Elapsed time 26.41s\n",
            "Epoch 34 Batch 420 Loss 0.1855 Elapsed time 26.61s\n",
            "Epoch 34 Batch 480 Loss 0.2519 Elapsed time 26.93s\n",
            "Epoch 34 Batch 540 Loss 0.2601 Elapsed time 26.71s\n",
            "Epoch 34 Batch 600 Loss 0.2217 Elapsed time 26.77s\n",
            "Epoch 34 Batch 660 Loss 0.2620 Elapsed time 26.91s\n",
            "Epoch 34 Batch 720 Loss 0.1891 Elapsed time 26.96s\n",
            "Epoch 34 Batch 780 Loss 0.2252 Elapsed time 26.92s\n",
            "Epoch 34 Batch 840 Loss 0.2352 Elapsed time 26.85s\n",
            "Epoch 34 Batch 900 Loss 0.1994 Elapsed time 26.81s\n",
            "Сайдә, яр башына менеп, якындагы куш каен төбенә барып сыенганчы, манма су булды .\n",
            "[[4165, 797, 317, 3973, 6495, 4992, 2621, 526, 133, 71701, 5726, 148, 79, 1]]\n",
            "на черную сторону, где стояла пара месте и не успела выбраться из дна воды\n",
            "Epoch 35 Batch 0 Loss 0.2232 Elapsed time 16.25s\n",
            "Epoch 35 Batch 60 Loss 0.2513 Elapsed time 26.89s\n",
            "Epoch 35 Batch 120 Loss 0.2427 Elapsed time 26.95s\n",
            "Epoch 35 Batch 180 Loss 0.2400 Elapsed time 26.90s\n",
            "Epoch 35 Batch 240 Loss 0.2041 Elapsed time 26.58s\n",
            "Epoch 35 Batch 300 Loss 0.2375 Elapsed time 26.39s\n",
            "Epoch 35 Batch 360 Loss 0.2023 Elapsed time 26.30s\n",
            "Epoch 35 Batch 420 Loss 0.1870 Elapsed time 26.19s\n",
            "Epoch 35 Batch 480 Loss 0.2527 Elapsed time 26.20s\n",
            "Epoch 35 Batch 540 Loss 0.2611 Elapsed time 26.20s\n",
            "Epoch 35 Batch 600 Loss 0.2209 Elapsed time 26.10s\n",
            "Epoch 35 Batch 660 Loss 0.2559 Elapsed time 26.09s\n",
            "Epoch 35 Batch 720 Loss 0.1894 Elapsed time 26.16s\n",
            "Epoch 35 Batch 780 Loss 0.2140 Elapsed time 26.08s\n",
            "Epoch 35 Batch 840 Loss 0.2283 Elapsed time 26.18s\n",
            "Epoch 35 Batch 900 Loss 0.2004 Elapsed time 26.54s\n",
            "Диләфрүз аның яхшылыкка чакырмаганын берьюлы төшенеп алды, ләкин ул әле кискен җавап бирергә әзер түгел иде .\n",
            "[[356, 11, 9910, 37680, 904, 3260, 664, 27, 3, 19, 1749, 143, 703, 819, 50, 4, 1]]\n",
            "диляфруз сказала не приглашает бы она еще чтобы ответить серьезно . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOc2pFAzP_wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "f1fa795b-2d38-41e6-cee8-f74630e45c46"
      },
      "source": [
        "test_sents = (\n",
        "    \"Ә чынында исә, кеше түгел, гөмбә ул!\",\n",
        "    \"Андагы пулемет һәм автомат тавышлары монда бик тонык ишетелә.\",\n",
        "    \"Озак та үтмәде, күбәләк-күбәләк кар ява башлады.\",\n",
        "    \"Күк йөзе, авыр болытлардан әкрен-әкрен генә арчылып, зәңгәр күл сыман, урман өстенә җәелде.\",\n",
        "    \"Меңнәрчә, миллионнарча кар энҗеләре җем-җем итеп балкыйлар.\",\n",
        "    \"Барыбыз да ул күрсәткән якка борылдык.\",\n",
        "    \"Шуннан соң без аның турында һичнәрсә ишетмәдек.\",\n",
        "    \"Аның тавышында мин бүгенгә кадәр һич тә ишетмәгән әллә нинди сагыш, әрнү сиздем һәм үземнең бу саксыз-лыгым өчен аңардан гафу үтендем.\",\n",
        "    \"Дөресен әйткәндә, сез икенче тапкыр дөньяга тудыгыз.\",\n",
        "    \"Ә Хәйбулланың үле гәүдәсе дә юк, бары тик бүреге һәм каскасы гына төшеп калган иде.\",\n",
        "    \"Сугышчылар, коралларына тотынып, тын да алмыйча көтә башладылар.\",\n",
        ")\n",
        "\n",
        "\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "    test_sequence = normalize_string(test_sent)\n",
        "    predict(test_sequence)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ә чынында исә, кеше түгел, гөмбә ул !\n",
            "[[17, 6101, 3205, 25, 62, 11695, 3, 9]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Андагы пулемет һәм автомат тавышлары монда бик тонык ишетелә .\n",
            "[[1552, 20203, 14, 4417, 1455, 88, 16, 1848, 938, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Озак та үтмәде, күбәләк-күбәләк кар ява башлады .\n",
            "[[190, 63, 3694, 20204, 409, 2810, 73, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Күк йөзе, авыр болытлардан әкрен-әкрен генә арчылып, зәңгәр күл сыман, урман өстенә җәелде .\n",
            "[[564, 3198, 182, 10528, 3697, 13, 20212, 487, 984, 889, 966, 183, 12462, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Меңнәрчә, миллионнарча кар энҗеләре җем-җем итеп балкыйлар .\n",
            "[[20215, 15419, 409, 20216, 8034, 48, 20217, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Барыбыз да ул күрсәткән якка борылдык .\n",
            "[[1746, 2, 3, 2055, 313, 10530, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Шуннан соң без аның турында һичнәрсә ишетмәдек .\n",
            "[[207, 60, 49, 11, 57, 20240, 15436, 1]]\n",
            "предмета предмета предмета предмета предмета смотр смотр смотр смотр смотр смотр штат предмета штат\n",
            "Аның тавышында мин бүгенгә кадәр һич тә ишетмәгән әллә нинди сагыш, әрнү сиздем һәм үземнең бу саксыз-лыгым өчен аңардан гафу үтендем .\n",
            "[[11, 2814, 10, 5501, 103, 258, 45, 5502, 61, 119, 9103, 6529, 5977, 14, 697, 8, 30506, 28, 954, 535, 9102, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Дөресен әйткәндә, сез икенче тапкыр дөньяга тудыгыз .\n",
            "[[787, 1342, 96, 135, 181, 794, 30554, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Ә Хәйбулланың үле гәүдәсе дә юк, бары тик бүреге һәм каскасы гына төшеп калган иде .\n",
            "[[17, 12537, 4741, 661, 5, 55, 151, 64, 7238, 14, 20436, 18, 292, 154, 4, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n",
            "Сугышчылар, коралларына тотынып, тын да алмыйча көтә башладылар .\n",
            "[[15457, 20451, 10605, 414, 2, 736, 927, 480, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwddMX_CDEFs",
        "colab_type": "text"
      },
      "source": [
        "### Infer using 20 training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZhEri8owee9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D_J4P-LDKsx",
        "colab_type": "text"
      },
      "source": [
        "### Infer on a random example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMsRRmu7m2wi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "779c1d11-b524-4916-f031-ba9364736af5"
      },
      "source": [
        "en_alignments, de_bot_alignments, de_mid_alignments, source, prediction = predict()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Малайлар да, Гүзәл дә шактый артта калдылар .\n",
            "[[1468, 26, 1587, 5, 156, 2124, 1148, 1]]\n",
            "предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета предмета\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV1_m_YjDPbL",
        "colab_type": "text"
      },
      "source": [
        "### Examine Encoder's attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzfCXDVXm6wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "796a03e0-5049-421a-b93e-9c2cdf49ea98"
      },
      "source": [
        "# attention = tf.reduce_mean(de_bot_alignments[0], axis=1).numpy()[0]\n",
        "attention = en_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + source)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, ''),\n",
              " Text(0, 0, 'Чөнки'),\n",
              " Text(0, 0, 'дөнья'),\n",
              " Text(0, 0, 'гыйлеме'),\n",
              " Text(0, 0, 'тупланган'),\n",
              " Text(0, 0, 'башка'),\n",
              " Text(0, 0, 'җир'),\n",
              " Text(0, 0, 'юк'),\n",
              " Text(0, 0, '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAJvCAYAAADGAY2LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCTYAgOH+f0+DwuDtCS5EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia__pcJ4DY-Z",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's lower attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRt7m8PmDdjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "791e7d11-d99b-4c67-e6f0-917f7418dea6"
      },
      "source": [
        "attention = de_bot_alignments[3][0, 7, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + prediction, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, ''),\n",
              " Text(0, 0, 'восемь'),\n",
              " Text(0, 0, 'же,'),\n",
              " Text(0, 0, 'как'),\n",
              " Text(0, 0, 'человек,'),\n",
              " Text(0, 0, 'можно'),\n",
              " Text(0, 0, 'отдать'),\n",
              " Text(0, 0, 'жизнь,'),\n",
              " Text(0, 0, 'но')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAJmCAYAAAD/x+zwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAwRBEDABjyfwBek4Xa3u54FwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AOVcy73DjWG",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's upper attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbT-_fYsDmLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "630d9b5b-93cb-42fc-aa00-54db4c1c1127"
      },
      "source": [
        "attention = de_mid_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, ''),\n",
              " Text(0, 0, 'когда'),\n",
              " Text(0, 0, 'раздался'),\n",
              " Text(0, 0, 'из'),\n",
              " Text(0, 0, 'больницы,'),\n",
              " Text(0, 0, 'их'),\n",
              " Text(0, 0, 'не'),\n",
              " Text(0, 0, 'в'),\n",
              " Text(0, 0, 'которой')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAGkCAYAAACmdojOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}